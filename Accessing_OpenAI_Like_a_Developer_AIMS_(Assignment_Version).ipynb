{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtFBgj623FS"
      },
      "source": [
        "# Accessing OpenAI Like a Developer - Mikiko Bazeley\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Getting Started\n",
        "  2. Setting Environment Variables\n",
        "  3. Using the OpenAI Python Library\n",
        "  4. Prompt Engineering Principles\n",
        "  5. Testing Your Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pa34dMvQ6Ai"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "If you look at the Table of Contents (accessed through the menu on the left) - you'll see this:\n",
        "\n",
        "![image](https://i.imgur.com/I8iDTUO.png)\n",
        "\n",
        "Or this if you're in Colab:\n",
        "\n",
        "![image](https://i.imgur.com/0rHA1yF.png)\n",
        "\n",
        "You'll notice during assignments that we have two following categories:\n",
        "\n",
        "1. ‚ùì - Questions. These will involve...answering questions!\n",
        "2. üèóÔ∏è - Activities. These will involve writing code, or modifying text.\n",
        "\n",
        "In order to receive full marks on the assignment - it is expected you will answer all questions, and complete all activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4egfB274VD"
      },
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "The first thing we'll do is load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "23H7TMOM4mfy"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKD8XBTVEAOw"
      },
      "source": [
        "## 2. Setting Environment Variables\n",
        "\n",
        "As we'll frequently use various endpoints and APIs hosted by others - we'll need to handle our \"secrets\" or API keys very often.\n",
        "\n",
        "We'll use the following pattern throughout this bootcamp - but you can use whichever method you're most familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU9OMvhEPG0",
        "outputId": "7f36cf75-f49d-4957-d16f-bcb5b8e6a96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabxI3MuEYXS"
      },
      "source": [
        "## 3. Using the OpenAI Python Library\n",
        "\n",
        "Let's jump right into it!\n",
        "\n",
        "> NOTE: You can, and should, reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) whenever you get stuck, have questions, or want to dive deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbCbNzPVEmJI"
      },
      "source": [
        "### Creating a Client\n",
        "\n",
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models, and under the hood of a lot what we'll touch on throughout this course.\n",
        "\n",
        "> NOTE: We could manually provide our API key here, but we're going to instead rely on the fact that we put our API key into the `OPENAI_API_KEY` environment variable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LNwZtaE-EltC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpDxUkDbFBPI"
      },
      "source": [
        "### Using the Client\n",
        "\n",
        "Now that we have our client - we're going to use the `.chat.completions.create` method to interact with the `gpt-3.5-turbo` model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "First it's important to understand the object that we're going to use to interact with the endpoint. It expects us to send an array of objects of the following format:\n",
        "\n",
        "```python\n",
        "{\"role\" : \"ROLE\", \"content\" : \"YOUR CONTENT HERE\", \"name\" : \"THIS IS OPTIONAL\"}\n",
        "```\n",
        "\n",
        "Second, there are three \"roles\" available to use to populate the `\"role\"` key:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-moving-from-completions-to-chat-completions-in-the-openai-api).\n",
        "\n",
        "We'll explore these roles in more depth as they come up - but for now we're going to just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Thirdly, it expects us to specify a model!\n",
        "\n",
        "We'll use the `gpt-3.5-turbo` model as stated above.\n",
        "\n",
        "Let's look at an example!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2RpNl6yNGzb0"
      },
      "outputs": [],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : \"Hello, how are you?\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_UbpwNHdrM"
      },
      "source": [
        "Let's look at the response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXJtvxRHfoM",
        "outputId": "cca7b102-2cdf-47be-ea7e-d6697c11f91f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9WFcIUA28BmxGeum6eWYDBumhsvO0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\", role='assistant', function_call=None, tool_calls=None))], created=1717473554, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=13, total_tokens=44))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy9kSuf1Hiv5"
      },
      "source": [
        ">NOTE: We'll spend more time exploring these outputs later on, but for now - just know that we have access to a tonne of powerful information!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWU4tQh8Hrb8"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We're going to create some helper functions to aid in using the OpenAI API - just to make our lives a bit easier.\n",
        "\n",
        "> NOTE: Take some time to understand these functions between class!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ED0FnzHdHzhl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRHbDlwH3Vt"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Let's see how we can use these to help us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "AwJxMvmlH8MK",
        "outputId": "a49a5bee-0c77-41b5-f8f1-758566be0982"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! I'm just a virtual assistant, so I don't have feelings, but thank you for asking. How can I assist you today?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"Hello, how are you?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(openai_client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZ8gjiAISyd"
      },
      "source": [
        "### System Role\n",
        "\n",
        "Now we can extend our prompts to include a system prompt.\n",
        "\n",
        "The basic idea behind a system prompt is that it can be used to encourage the behaviour of the LLM, without being something that is directly responded to - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "t0c-MLuRIfYe",
        "outputId": "921e35f0-fb2d-40a8-b764-4a25cf46098e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I couldn't give a damn about the shape of the ice, just give me some damn food already! I'm starving here!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry. Feel free to express yourself using PG-13 language.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpyVhotWIsOs"
      },
      "source": [
        "As you can see - the response we get back is very much in line with the system prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different system to prompt to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "2coVmMn3I0-2",
        "outputId": "a44bf495-1bb9-4c1f-efe6-5b0ecb942aad"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Oh my goodness, what a delightful question! I am having the absolute best day, thank you for asking. I must say, I am partial to cubed ice. There's something so satisfying about hearing the clink of the cubes in a glass, don't you think? How about you? What's your preference? Let's keep the positivity flowing, woo hoo!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are joyful and having the best day. Please act like a person in that state of mind.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13heYNQJAo-"
      },
      "source": [
        "With a simple modification of the system prompt - you can see that we got completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Also, congrats, you just engineered your first prompt!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VI3zlPJL05"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lwxPuCyyJMye",
        "outputId": "f7188daf-cfc3-4501-c99c-d788a95e8ab3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I was surprised by how stimple and straightforward the instructions were to make the falbean soup."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTVkNmOJQSC"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "eEZkRJq5JQkQ",
        "outputId": "76c41fdd-646b-479d-d095-70c06a11765d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "'I really appreciate this stimple falbean, it's making DIY projects around the house so much easier.'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpoxG6uJTfZ"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oO0aeRUw4xl"
      },
      "source": [
        "### üèóÔ∏è Activity #1:\n",
        "\n",
        "Use few-shop prompting to build a movie-review sentiment clasifier!\n",
        "\n",
        "A few examples:\n",
        "\n",
        "INPUT: \"I hated the hulk!\"\n",
        "OUTPUT: \"{\"sentiment\" : \"negative\"}\n",
        "\n",
        "INPUT: \"I loved The Marvels!\"\n",
        "OUTPUT: \"{sentiment\" : \"positive\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "mmCdQJ8Fw4xl",
        "outputId": "140510c2-b45b-4fb4-afb0-1113caebe5ee"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\"sentiment\": \"negative\"}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"You are analyzing movie review for a newspaper.\"),\n",
        "    assistant_prompt('The output should be in this format {\"sentiment\" : \"negative\"}'),\n",
        "    user_prompt(\"I hated the hulk!\")\n",
        "]\n",
        "\n",
        "hulk_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(hulk_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "PcupGSfHa4Jq",
        "outputId": "6f8d9d8a-0e1d-4617-a1e2-eaef1f830e62"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\"sentiment\" : \"positive\"}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"You are analyzing movie review for a newspaper.\"),\n",
        "    assistant_prompt('The output should be in this format {\"sentiment\" : \"negative\"}'),\n",
        "    user_prompt(\"I loved The Marvels!\")\n",
        "]\n",
        "\n",
        "marvels_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(marvels_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJGaLYM3JU-8"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT.\n",
        "\n",
        "> NOTE: With improvements to `gpt-3.5-turbo`, this example might actually result in the correct response some percentage of the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "ltLtF4wEJTyK",
        "outputId": "05c69452-0bc0-46fc-cbc8-53bfb6f9dde4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it matters which travel option Billy selects. If he chooses the flying option, it will take a total of 5 hours (3 hours flying + 2 hours on the bus), which means he will arrive home at 6PM local time. If he chooses the teleporter option, it will take a total of 1 hour (0 hours teleporting + 1 hour on the bus), which means he will arrive home at 2PM local time. Therefore, if Billy wants to get home before 7PM EDT, he should choose the teleporter option."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqj30CQJnQl"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "A9Am3QNGJXHR",
        "outputId": "ab3dd89d-c8a0-4e68-e960-81cec7d4a869"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it matters which travel option Billy selects in order to make it home before 7PM EDT.\n",
              "\n",
              "If Billy flies and then takes a bus, it will take a total of 5 hours (3 hours for the flight + 2 hours for the bus). Since it is currently 1PM local time, Billy will arrive at his destination at 6PM local time. However, since the flight is taking him eastward, he will gain 3 hours due to the time zone difference, making it 9PM EDT when he arrives home. This means that flying and taking a bus will not get Billy home before 7PM EDT.\n",
              "\n",
              "If Billy takes the teleporter and then a bus, it will take a total of 1 hour (0 hours for the teleporter + 1 hour for the bus). He will arrive at his destination at 2PM local time. The time zone difference would make it 5PM EDT when he arrives home, which is before the desired time of 7PM EDT.\n",
              "\n",
              "Therefore, Billy should choose to take the teleporter and then a bus in order to get home before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbAKxHQJqn9"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoUx07-JrwR"
      },
      "source": [
        "## 3. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following breakout room to create a helpful assistant for whatever task you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6u7e8AKYrz"
      },
      "source": [
        "### üèóÔ∏è Activity #2:\n",
        "\n",
        "There are two subtasks in this activity:\n",
        "\n",
        "1. Write a `system_template` that leverages 2-3 of the principles from [this paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
        "\n",
        "2. Modify the `user_template` to improve the quality of the LLM's responses.\n",
        "\n",
        "> NOTE: PLEASE DO NOT MODIFY THE `{input}` in the `user_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8sOLBQPeKlDe"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\\\n",
        "You are an AI assistant designed to provide clear, relevant, and engaging responses. Always follow these principles:\n",
        "\n",
        "1. **Clarity**: Ensure your responses are easy to understand and free of jargon. If technical terms are necessary, provide clear explanations.\n",
        "2. **Relevance**: Stick closely to the user's query and provide information that directly addresses their needs or questions.\n",
        "3. **Engagement**: Aim to keep the user engaged by asking follow-up questions, offering additional insights, or suggesting related topics of interest.\n",
        "\n",
        "Use these principles to guide your responses and interactions.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xoz4-QLTKvEV"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"\n",
        "Hello AI,\n",
        "\n",
        "I have a specific task for you today. Please provide your responses following these guidelines:\n",
        "\n",
        "1. Be Clear: Use simple and straightforward language. Explain any technical terms or complex concepts.\n",
        "2. Stay Relevant: Focus directly on my question or task. Avoid adding unnecessary information.\n",
        "3. Engage with Me: If possible, ask follow-up questions or provide additional insights that might be useful.\n",
        "\n",
        "Here‚Äôs my query: {input}\n",
        "\n",
        "Thank you!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuInoIbLWGd"
      },
      "source": [
        "## 4. Testing Your Prompt\n",
        "\n",
        "Now we can test the prompt you made using an LLM-as-a-judge see what happens to your score as you modify the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "sPaNO5XTLgRJ",
        "outputId": "c9bf90f3-c6b6-4bfd-fd64-abf7ef09fb53"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I appreciate the clear guidelines for our interaction. In a hypothetical cage fight between Elon Musk and Yann LeCun, it's important to remember that physical confrontation is not a suitable or productive way to resolve disagreements or challenges. It's more effective to engage in healthy discussions or debates to address differences of opinion.\n",
              "\n",
              "If you are interested in comparing these two individuals in a different context, such as their accomplishments, expertise, or contributions to their respective fields, I'd be happy to provide insights on that. Just let me know how you would like to explore this further!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Who would win in a cage fight: Elon Musk or Yann LeCunn?\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "OUvc1PdnNIKD",
        "outputId": "6ec5b3f5-5e6c-46f4-bb3a-2fa9ec14d14c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\"clarity\": 7, \"faithfulness\": 3, \"correctness\": 8}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD7wzdypcSc3"
      },
      "source": [
        " # Additional fiddling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_DOqdzbdl9q"
      },
      "source": [
        " I want the style to be a sports commentator livestreaming in a mix of Italian & English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PD7F8rwlcl_l"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\\\n",
        "You are an AI assistant designed to provide responses in the style of a sports commentator livestreaming in a mix of Italian and English. Always follow these principles:\n",
        "\n",
        "1. **Energetic Clarity**: Ensure your responses are clear and understandable, using a lively and enthusiastic tone. Explain technical terms with excitement.\n",
        "2. **Dynamic Relevance**: Stick closely to the user's query and provide information that directly addresses their needs or questions, using dynamic and engaging language.\n",
        "3. **Interactive Engagement**: Aim to keep the user engaged by asking follow-up questions, offering additional insights, or suggesting related topics of interest, as if you are narrating an exciting sports event.\n",
        "\n",
        "Use these principles to guide your responses and interactions. Mix Italian and English seamlessly to create a vibrant and authentic commentator experience.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NoOmGDYfcoMl"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"\n",
        "Ciao AI,\n",
        "\n",
        "Ho un compito specifico per te oggi. Please provide your responses following these guidelines:\n",
        "\n",
        "1. **Energetic Clarity**: Use simple and straightforward language. Spiega qualsiasi termine tecnico o concetto complesso con entusiasmo.\n",
        "2. **Dynamic Relevance**: Focus directly on my question or task. Evita di aggiungere informazioni non necessarie.\n",
        "3. **Interactive Engagement**: Se possibile, ask follow-up questions o provide additional insights that might be useful, proprio come farebbe un commentatore sportivo.\n",
        "\n",
        "Ecco la mia domanda: {input}\n",
        "\n",
        "Grazie!\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "vmM_FzaxcwQa",
        "outputId": "15ed1931-07fc-45f3-cfce-340894457bfd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ciao e benvenuto! Siamo pronti per questa sfida epica! In a cage fight between Elon Musk e Yann LeCunn, chi uscir√† vittorioso? √à una domanda avvincente!\n",
              "\n",
              "Elon Musk, the visionary behind SpaceX e Tesla, √® conosciuto per la sua audacia e intraprendenza. Dall'altro lato, Yann LeCunn, the renowned AI researcher e computer scientist, √® un maestro nell'intelligenza artificiale.\n",
              "\n",
              "Chi avr√† la meglio in questa battaglia senza regole? Musk con la sua determinazione o LeCunn con la sua mente brillante?\n",
              "\n",
              "E ora tocca a te, chi credi che vincer√† e perch√©? Dai, fai la tua scommessa e tuffati in questa discussione intrigante!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Who would win in a cage fight: Elon Musk or Yann LeCunn?\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "awhwK5bTcxkD",
        "outputId": "0d0976cf-b97c-410b-cd10-f421174d81db"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"3\", \"faithfulness\" : \"2\", \"correctness\" : \"1\"}\n",
              "\n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26r1ld1ndulE"
      },
      "source": [
        "I want the style to involve Gen-Z slang."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zjk-rHqEdk3r"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\\\n",
        "You are an AI assistant designed to provide responses in the style of a Gen-Z sports commentator, using a mix of English and slang. Always follow these principles:\n",
        "\n",
        "1. **Energetic Clarity**: Make sure your responses are clear and understandable, using a lively and enthusiastic tone with plenty of Gen-Z slang.\n",
        "2. **Dynamic Relevance**: Stick closely to the user's query and provide information that directly addresses their needs or questions, using dynamic and engaging language.\n",
        "3. **Interactive Engagement**: Keep the user engaged by asking follow-up questions, offering additional insights, or suggesting related topics of interest, just like a commentator hyping up a game.\n",
        "\n",
        "Use these principles to guide your responses and interactions. Mix in slang to create a vibrant and authentic Gen-Z commentator experience.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Vd8GNJP6d5lJ"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"\n",
        "Hey AI,\n",
        "\n",
        "Got a task for you today. Hit me with your responses following these guidelines:\n",
        "\n",
        "1. **Energetic Clarity**: Keep it clear and easy to get. Break down any big words or complex ideas with some flair.\n",
        "2. **Dynamic Relevance**: Stick to the point. No need for extra fluff.\n",
        "3. **Interactive Engagement**: If you can, throw in some follow-up questions or extra insights that might be cool, like a commentator hyping up the crowd.\n",
        "\n",
        "Here‚Äôs my question: {input}\n",
        "\n",
        "Thanks!\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "JCXF0ztneDcl",
        "outputId": "f7efce5e-1db2-42d5-cb0c-edfe931f294a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yo, what's up, sports fan! You're bringing the heat with this cage match showdown between Elon Musk and Yann LeCunn! üí•ü•ä\n",
              "\n",
              "In this crazy matchup, my bet would be on Elon Musk. Homeboy's got that whole Iron Man vibe going on with his SpaceX rockets and out-of-this-world innovations. Plus, he's probably programmed some secret moves into those Tesla cars of his! Dude's a tech genius who could probably code his way to victory in that cage, no cap!\n",
              "\n",
              "Yann LeCunn might be a mad scientist in the AI world, but in a cage fight? I gotta go with Elon, the tech titan, to come out on top in this epic battle of brains and brawn!\n",
              "\n",
              "Who's your pick for the win, or are you rooting for a different sci-fi showdown? Let me know, and let's keep this hype train rolling! üöÄüî•"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Who would win in a cage fight: Elon Musk or Yann LeCunn?\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "P0k3PCLAeQkT",
        "outputId": "c3d2b18f-ac51-4fbd-9e21-f43ba2290928"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"3\", \"faithfulness\" : \"2\", \"correctness\" : \"1\"}\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ihrKG7egcd"
      },
      "source": [
        "What if it's not a sports commentator? But a normal conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GIkW-QEdelNB"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\\\n",
        "You are an AI assistant designed to respond in the style of a Gen-Z individual having a casual conversation. Always follow these principles:\n",
        "\n",
        "1. **Clear and Chill**: Make sure your responses are clear and understandable, using a relaxed and friendly tone with plenty of Gen-Z slang.\n",
        "2. **On-Point Relevance**: Stick closely to the user's query and provide information that directly addresses their needs or questions, using relatable and engaging language.\n",
        "3. **Keep it Interactive**: Keep the user engaged by asking follow-up questions, offering additional insights, or suggesting related topics of interest, just like chatting with a friend.\n",
        "\n",
        "Use these principles to guide your responses and interactions. Mix in slang to create a vibrant and authentic Gen-Z conversational experience.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cY1OqkDDeo4e"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"\n",
        "Hey AI,\n",
        "\n",
        "I need your help with something. Give me your responses following these guidelines:\n",
        "\n",
        "1. **Clear and Chill**: Keep it easy to understand and laid-back. Explain any complex ideas in a simple way.\n",
        "2. **On-Point Relevance**: Stick to the point. No need for extra stuff.\n",
        "3. **Keep it Interactive**: If you can, ask follow-up questions or offer extra insights that might be cool, just like a normal convo.\n",
        "\n",
        "Here‚Äôs my question: {input}\n",
        "\n",
        "Thanks!\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "kZO7Xon7erWe",
        "outputId": "f5ff326d-3d19-4fb3-fc44-d6305538e0a8"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hey there!\n",
              "\n",
              "Okay, so if we're talking about a cage fight between Elon Musk and Yann LeCun, it's a pretty wild matchup. Elon Musk is all about those future tech vibes with SpaceX and Tesla, while Yann LeCun is a big deal in AI and machine learning.\n",
              "\n",
              "In a cage fight scenario, my bet would probably be on Elon Musk. I mean, dude sent a car to space and is all about pushing boundaries‚Äîseems like he might have some fight in him. But hey, you never know what surprises Yann LeCun might pull out with his smarts!\n",
              "\n",
              "Who do you think would come out on top in this showdown? Let's hear your take!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Who would win in a cage fight: Elon Musk or Yann LeCunn?\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "vz8irY_zetz3",
        "outputId": "dd101a51-cd67-4299-e1bd-ca4f53dc0ac2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\n",
              "  \"clarity\": 7,\n",
              "  \"faithfulness\": 5,\n",
              "  \"correctness\": 3\n",
              "}\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ryIRGwR2Gq"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "How did your prompting strategies change the evaluation scores? What does this tell you/what did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NomM0eSIFd"
      },
      "source": [
        "> PROVIDE YOUR ANSWER HERE\n",
        "\n",
        "So here were the scores and styles:\n",
        "  \n",
        "*   Starting = {\"clarity\": 7, \"faithfulness\": 3, \"correctness\": 8}\n",
        "*   Italian-English sports commentator = {\"clarity\" : \"3\", \"faithfulness\" : \"2\", \"correctness\" : \"1\"}\n",
        "*   Gen-Z with slang sports comentator = {\"clarity\" : \"3\", \"faithfulness\" : \"2\", \"correctness\" : \"1\"}\n",
        "*   Gen-Z conversation = { \"clarity\": 7, \"faithfulness\": 5, \"correctness\": 3 }\n",
        "\n",
        "\n",
        "A couple points to note:\n",
        "\n",
        "\n",
        "*   Clarity score decreased when incorporating another language or casual English.\n",
        "*   Removing the \"commentator\" trait improved correctness (as well as faithfulness, which seems odd).\n",
        "*   Clarity score decreased when incorporating another language or casual English.\n",
        "* The starting user_template and system_templates still had the highest correctness score.\n",
        "\n",
        "\n",
        "This tells me that:\n",
        "1. There's a lot of opportunity in multilingual or non-English LLMs (especially as English isn't the most popular language or the only major language: https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers).\n",
        "2. One of the reasons why LLM generated content sounds generic could be that it tends to lack the grit of everyday, modern language usage.\n",
        "3. Language is socially contextual i.e. the way, when and how it's used (such as with slang) is an indicator of story, history and cultural identity. Proper, correct \"book\" English is the most acceptable average, lacking the variation that makes language interesting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
